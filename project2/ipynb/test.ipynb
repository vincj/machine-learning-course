{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My example - Arrange csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fileinput\n",
    "\n",
    "# DON'T FORGET TO REMOVE HEADER Id,Prediction\n",
    "with fileinput.FileInput(\"data/data_train_woHeader.csv\", inplace=True, backup='.txt') as file: # sampleSubmission_woHeader\n",
    "    for line in file:\n",
    "        print(line.replace(\"_\", \",\"), end='')\n",
    "        \n",
    "with fileinput.FileInput(\"data/data_train_woHeader.csv\", inplace=True, backup='.txt') as file:\n",
    "    for line in file:\n",
    "        print(line.replace(\"r\", \"\"), end='')\n",
    "        \n",
    "with fileinput.FileInput(\"data/data_train_woHeader.csv\", inplace=True, backup='.txt') as file:\n",
    "    for line in file:\n",
    "        print(line.replace(\"c\", \"\"), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My example - Load submissionSample.csv for prediction by ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"ALSExample\")\\\n",
    "        .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"data/sampleSubmission_woHeader.csv\").rdd \n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[0]), userId=int(p[1]), \n",
    "                                     rating=float(p[2])))\n",
    "submission_ratings = spark.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My example - Use pyspark ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"ALSExample\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "lines = spark.read.text(\"data/data_train_woHeader.csv\").rdd \n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[0]), userId=int(p[1]), \n",
    "                                     rating=float(p[2])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(rank=20, maxIter=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")  \n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                    predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.1231989279282748\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "# $example off$\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My example - Prepare file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save DataFrame as csv\n",
    "pred_cv.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"data/sampleSubmission_new_to_be_processed\") # predictions.write.csv('data/pred_als_spark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deal_line(line):\n",
    "    itemId, rating, userId, pred = line.split(',') # such is the ordering in the csv file created from \n",
    "    return int(itemId), int(userId), round(float(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"data/sampleSubmission_new_to_be_processed/sampleSubmission_new_to_be_processed.csv\", \"r\") as f:\n",
    "    data = f.read().splitlines()\n",
    "    data_transit = [deal_line(line) for line in data[1:]]\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data/sampleSubmission_als_20_10_cv_1.csv\", 'w') as csvfile:\n",
    "    fieldnames = ['Id', 'Prediction']\n",
    "    writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    data_transit.sort(key=lambda line: (line[1],line[0])) # order the coordinates by columns\n",
    "    for item_ in data_transit:\n",
    "        writer.writerow({'Id':'r'+'{}'.format(item_[0])+'_c'+'{}'.format(item_[1]),'Prediction':float(item_[2])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/sampleSubmission_als_20_10_cv_1.csv\",\"r\") as f:\n",
    "    reader = csv.reader(f,delimiter = \",\")\n",
    "    data = list(reader)\n",
    "    row_count = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My example - Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als_perso = ALS(rank=20, maxIter=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder().build() #\\\n",
    "                    #.addGrid(als_perso.rank, [8, 12]) \\\n",
    "                    #.addGrid(als_perso.maxIter, [10, 15]) \\\n",
    "                    #.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                    predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=als_perso,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = crossval.fit(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_cv = model.transform(submission_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(pred_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8259579478838837"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176952"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
